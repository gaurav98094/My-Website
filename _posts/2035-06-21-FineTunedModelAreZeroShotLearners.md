---
layout: post
title: "FineTuned Language Models Are Zero-Shot Learners"
date: 2025-06-21
categories: [papers]
tags: [llm]
description: "FineTuned Language Models Are Zero-Shot Learners"
image: /images/image1.png
---

<img src="{{ '/poster/finetunemodelarezeroshot.png' | relative_url }}">


# FineTuned Language Models Are Zero-Shot Learners

ğŸ”— [**Original Paper (Sennrich et al., 2015)**](https://arxiv.org/pdf/2109.01652)


## Abstract

* Proposes **instruction tuning**: finetuning pretrained LMs on >60 NLP tasks phrased as naturalâ€‘language instructions.
* Produces **FLAN**, a 137Bâ€‘parameter model.
* On **unseen tasks**, FLAN beats zeroâ€‘shot GPTâ€‘3 (175B) on 20/25 datasets and even fewâ€‘shot GPTâ€‘3 on benchmarks like ANLI, RTE, BoolQ, AI2â€‘ARC, OpenBookQA, and StoryCloze.
* Finds that **task diversity**, **model scale**, and **instruction usage** are critical drivers.


## Introduction

* Highlights the limitations of zeroâ€‘shot performance in large LMs like GPTâ€‘3.
* Motivates converting tasks into **naturalâ€‘language instruction templates** (e.g., â€œTranslate: ...â€, â€œIs it positive or negative?â€).
* Presents the concept of **instruction tuning** to align LMs with human-style instructions.


## Model Architecture

* Uses a **decoder-only Transformer** model (~137B params), pretrained on diverse data (web text, code, dialogue, multilingual).
* The base model (â€œLaMDAâ€‘PTâ€) is similar in scale to GPTâ€‘3, but with different pretraining corpora.


## Instructionâ€¯Tuning (FLAN)

* Aggregates 62 datasets from TensorFlow Datasets across **12 task clusters** (e.g., translation, NLI, commonsense, reading comprehension).
* Converts each dataset into **instructionâ€‘input â†’ output** pairs using 10 templates per dataset cluster.
* Finetunes with up to 30,000 examples per dataset, balanced via proportional mixing and gradient updates (total ~30k steps).


## Evaluation & Results

* **Unseen-task protocol**: hold out entire task clusters during tuning to test on zeroâ€‘shot generalization.
* **Comparisons**: base model vs FLAN vs zeroâ€‘fewâ€‘shot GPTâ€‘3.
* **Findings**:

  * FLAN surpasses zeroâ€‘shot GPTâ€‘3 on 20/25 tasks.
  * Outperforms fewâ€‘shot GPTâ€‘3 on key benchmarks such as ANLI, RTE, BoolQ, AI2â€‘ARC, OpenBookQA, StoryCloze.
  * Benefits consistent across task clusters: NLI, QA types, etc.


## Ablation Studies

* **Scale Effect**: models of 422M, 2B, 8B, 68B, 137B params tested. Instruction tuning yields significant gains primarily at larger scales (â‰¥8Bâ€“137B).
* **Task diversity**: increasing number of tuning tasks improves zeroâ€‘shot performance on unseen tasks.
* **Instruction necessity**: removing templates or replacing them with dataset names degrades performance significantly.
* **Fewâ€‘shot exemplar augmentation**: adding a few examples within instructions further boosts performance and stabilizes results.
* **Prompt tuning compatibility**: FLAN serves as a better base for continuous prompt tuning, achieving strong performance with few labeled examples .



## Implementation Details

* Datasets limited to 30k examples each (others capped proportionally).
* Finetuning config: 30k gradient updates, batch size 8,192, learning rate 3eâ€‘5, sequence lengths 1,024/256, using Adafactor optimizer.
* Training is efficient: only ~2% extra compute over pretraining .



## Discussion & Conclusion

* Instruction tuning effectively bridges prompting and fineâ€‘tuning approaches.
* FLAN showcases strong zeroâ€‘shot capabilities by learning to "follow instructions."
* Shows that labeled data can augment pretrained LMs beyond singleâ€‘task fineâ€‘tuning.
* Suggests future directions: broader task clusters, crossâ€‘lingual tuning, reducing bias, behavior control .


## Ethical & Environmental Considerations

* **Bias risk**: tasks and templates may embed social biases, which instructionâ€‘tuned models could amplify.
* **Accessibility tradeâ€‘off**: easier-to-use models have pros and cons regarding misuse.
* **Energy impact**: FLAN uses \~2% more compute postâ€‘pretraining (\~451â€¯MWh for base LM, carbon \~26â€¯tCOâ‚‚e) .



## Summary

FLAN, a 137Bâ€‘parameter model trained with **instruction tuning** on over 60 tasks, substantially enhances zeroâ€‘shot performanceâ€”outperforming zeroâ€‘ and fewâ€‘shot GPTâ€‘3 on most benchmarks. Core contributions include demonstrating that **model scale**, **instruction diversity**, and **structured tasks** are key, while also showing compatibility with fewâ€‘shot prompting and prompt tuning.
