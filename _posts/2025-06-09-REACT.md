---
layout: post
title: "ReAct: Reasoning and Acting"
date: 2025-06-09
categories: papers
tags: [llm, prompting]
description: "Paradigm where agent uses chain-of-thought reasoning and tool-using actions in aggregation."
image: /images/image1.png
---

<img src="{{ '/poster/react.png' | relative_url }}">



# REACT: SYNERGIZING REASONING AND ACTING IN LLM

üîó [**Original Paper**](https://arxiv.org/pdf/2210.03629)  <br>
üìù [**Implementing React From Scratch**](https://github.com/gaurav98094/AI_Agents/blob/main/Part%204/01-ReActFromScratch.ipynb)



## üìù Abstract

This paper explores the use of Large Language Models (LLMs) as agents that can both **reason** and **act** to solve complex, multi-step tasks.

* It introduces **ReAct**: a framework where LLMs generate two kinds of outputs:

  * üß† **Reasoning traces** ‚Äî Natural language thoughts that help the model **decompose tasks**, **track progress**, and **adapt action plans**.
  * ‚öôÔ∏è **Task-specific actions** ‚Äî Structured commands or API calls that allow the model to **interact with external tools** and **gather real-time information**.

* Unlike traditional **Chain-of-Thought (CoT)** prompting‚Äîwhich relies solely on internal reasoning‚ÄîReAct enables LLMs to:

  * Dynamically query external sources (e.g. tools, APIs, databases)
  * Incorporate feedback into their next steps
  * Update their reasoning with fresh context

* üîç **Key benefit:**
  ReAct mitigates **hallucinations** and **error propagation** often observed in CoT by grounding reasoning in **observable outcomes** and **environment feedback**.

* The result is a new class of LLM-based agents capable of **goal-driven planning**, **adaptive decision-making**, and **interactive learning** in open-ended settings.


## üß† Introduction

* Humans excel at solving new tasks by **interleaving reasoning and acting**‚Äîwe think, act, observe, then adjust our thinking. This tight feedback loop enables **rapid learning** and **robust decision-making**, even in unfamiliar or uncertain environments.

<img src="{{ '/images/react2.png' | relative_url }}">

* Traditional prompting methods like **Chain-of-Thought (CoT)** rely on purely internal reasoning. While useful, CoT behaves like a **static black box**‚Äîthe model "thinks" using its internal knowledge but does **not interact with the environment** during reasoning. This limits its effectiveness in dynamic or interactive tasks.

* Prior work has focused mainly on **simple question answering or embodied agents in toy environments**, without fully exploring what happens when **reasoning and acting are combined** for complex, multi-step tasks.

* **ReAct (Reason + Act)** introduces a powerful paradigm: prompting large language models (LLMs) to produce both:

  * **Language-based thoughts** (reasoning traces) and
  * **Environment-altering actions**,
    in a **step-by-step, interleaved manner**.

* This joint reasoning‚Äìacting cycle enables:

  * Task decomposition
  * Dynamic replanning
  * Information gathering and hypothesis testing
  * Handling unexpected situations

* üß© **The best of both worlds:** Combining **ReAct** with **CoT** enables the model to leverage both:

  * Its **internal knowledge** (via reasoning), and
  * **External feedback** (via actions and observations)
    to build more **grounded**, **adaptive**, and **generalizable** task-solving agents.

## ReAct Maths

<img src="{{ '/images/react3.png' | relative_url }}">
<img src="{{ '/images/react4.png' | relative_url }}">
<img src="{{ '/images/react5.png' | relative_url }}">
<img src="{{ '/images/react6.png' | relative_url }}">


### ‚úÖ Summary

* ReAct augments the action space to include **internal thoughts**.
* Language thoughts help in:

  * Planning
  * Injecting commonsense
  * Decomposing goals
  * Adjusting to feedback
* Despite the challenge of infinite L, a frozen LLM with carefully designed few-shot examples can **learn to reason and act** effectively in complex tasks.


## Results
<img src="{{ '/images/react7.png' | relative_url }}">
