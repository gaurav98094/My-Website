---
layout: post
title: "BERT — Bidirectional Transformers for Language Understanding"
date: 2025-06-08
categories: papers
tags: [bert, pretraining, masked-language-modeling, nlp]
description: "A deep dive into BERT, the transformer model that revolutionized language understanding."
image: /images/image1.png
---

<img src="{{ '/poster/bert.png' | relative_url }}">



# 📘 BERT: Bidirectional Encoder Representations from Transformers

🔗 [**Original Paper**](https://arxiv.org/pdf/1810.04805)  <br>
📝 [**Illustrated Blog** by Jay Alammar](https://jalammar.github.io/illustrated-bert/)


## 🧠 **Abstract**

* BERT is designed to **pre-train deep bidirectional representations** from unlabeled text by jointly conditioning on both **left and right context** in all layers.
* It achieves **state-of-the-art performance** on a wide range of NLP tasks by **fine-tuning** with just one additional output layer.
* Demonstrated top performance on **11 NLP benchmarks**, proving the power of deep, bidirectional contextual language modeling.

---

## 🧩 **Introduction**

1. **Language model pre-training** has significantly improved performance on both **sentence-level tasks** (e.g., paraphrasing, natural language inference) and **token-level tasks** (e.g., named entity recognition, question answering).

2. Two primary strategies for leveraging pre-trained models:

   * **Feature-based**: Uses fixed representations as features (e.g., **ELMo**).
   * **Fine-tuning**: Updates the entire model on the downstream task (e.g., **GPT**).

3. A limitation of earlier models like GPT is that they are **unidirectional** (typically left-to-right), which limits their contextual understanding.

4. Many tasks require **bidirectional context** for optimal performance, which unidirectional models can't provide.

5. **BERT** introduces **bidirectional training** using a **Masked Language Model (MLM)** objective, enabling the model to predict masked tokens using both left and right context.

6. BERT also incorporates **Next Sentence Prediction (NSP)** to model relationships between sentence pairs—critical for tasks like **natural language inference** and **QA**.

7. Unlike previous models, BERT performs **deep bidirectional pre-training** within a single architecture, significantly improving contextual representation.

8. BERT reduces the need for complex, task-specific architectures—just add a simple output layer and fine-tune on the downstream task.

9. With this approach, BERT achieves **state-of-the-art results across 11 NLP tasks**, including GLUE, SQuAD, and more.

---

## 🔍 **Related Work**

### 2.1 🔧 Unsupervised Feature-based Approaches

* Learn fixed or contextual embeddings from unlabeled text **without fine-tuning**.
* Examples:

  * **Non-neural**: Brown Clustering.
  * **Neural**: Word2Vec, GloVe.
  * **Sentence-level**: Skip-Thought, Paragraph Vector.
  * **Contextual**: ELMo (context-sensitive using bidirectional LMs).
* Widely adopted to improve downstream NLP performance via pre-trained embeddings.

### 2.2 🧪 Unsupervised Fine-tuning Approaches

* Pretrain language models on unlabeled data and **fine-tune** the entire model for specific tasks.
* Examples: **GPT**, **ULMFiT**.
* Benefits:

  * Reduces the need for large task-specific models.
  * Achieves high accuracy with fewer labeled examples.
  * Strong benchmark performance (e.g., **GLUE**).

### 2.3 🔁 Transfer Learning from Supervised Data

* Leverages large **supervised datasets** (e.g., NLI, translation) to learn reusable representations.
* Transfers the learned knowledge to other tasks.
* Examples:

  * **NLP**: InferSent, CoVe.
  * **Vision**: ImageNet-pretrained CNNs.
* Effective when high-quality labeled data is available.

---

## 🧱 **BERT Architecture**

### 🔄 Two-Stage Framework

1. **Pre-training**: Train on large amounts of **unlabeled text** using:

   * Masked Language Modeling (MLM)
   * Next Sentence Prediction (NSP)
2. **Fine-tuning**: Initialize with pre-trained weights and **fine-tune** on labeled task-specific data. 🔁 **One model per task.**

<img src="{{ '/images/bert1.png' | relative_url }}">

### 🔍 Key Features

* **Unified architecture** across all NLP tasks.
* Eliminates need for task-specific design—just fine-tune with minimal modifications.

### 🧬 Model Configuration
<img src="{{ '/images/bert2.png' | relative_url }}">

* BERT-Base has the same size as the original **OpenAI GPT** model.
* Uses **WordPiece tokenizer** with a 30,000-token vocabulary.

### 🔡 Input Embedding Composition

InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding

<img src="{{ '/images/bert3.png' | relative_url }}">


