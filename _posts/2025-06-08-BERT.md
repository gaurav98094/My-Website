---
layout: post
title: "BERT — Bidirectional Transformers for Language Understanding"
date: 2025-06-08
categories: papers
tags: [bert, pretraining, masked-language-modeling, nlp]
description: "A deep dive into BERT, the transformer model that revolutionized language understanding."
image: /images/image1.png
---

<img src="{{ '/poster/bert.png' | relative_url }}">



# 📘 BERT: Bidirectional Encoder Representations from Transformers

🔗 [**Original Paper**](https://arxiv.org/pdf/1810.04805)  <br>
📝 [**Illustrated Blog** by Jay Alammar](https://jalammar.github.io/illustrated-bert/)


## 🧠 Abstract

* BERT is designed to **pre-train deep bidirectional representations** from unlabeled text by jointly conditioning on both **left and right context** in all layers.
* It achieves **state-of-the-art performance** on a wide range of NLP tasks by **fine-tuning** with just one additional output layer.
* Demonstrated top performance on **11 NLP benchmarks**, proving the power of deep, bidirectional contextual language modeling.

---

## 🧩 Introduction

1. **Language model pre-training** has significantly improved performance on both **sentence-level tasks** (e.g., paraphrasing, natural language inference) and **token-level tasks** (e.g., named entity recognition, question answering).

2. Two primary strategies for leveraging pre-trained models:

   * **Feature-based**: Uses fixed representations as features (e.g., **ELMo**).
   * **Fine-tuning**: Updates the entire model on the downstream task (e.g., **GPT**).

3. A limitation of earlier models like GPT is that they are **unidirectional** (typically left-to-right), which limits their contextual understanding.

4. Many tasks require **bidirectional context** for optimal performance, which unidirectional models can't provide.

5. **BERT** introduces **bidirectional training** using a **Masked Language Model (MLM)** objective, enabling the model to predict masked tokens using both left and right context.

6. BERT also incorporates **Next Sentence Prediction (NSP)** to model relationships between sentence pairs—critical for tasks like **natural language inference** and **QA**.

7. Unlike previous models, BERT performs **deep bidirectional pre-training** within a single architecture, significantly improving contextual representation.

8. BERT reduces the need for complex, task-specific architectures—just add a simple output layer and fine-tune on the downstream task.

9. With this approach, BERT achieves **state-of-the-art results across 11 NLP tasks**, including GLUE, SQuAD, and more.

---

## 🔍 Related Work

### 2.1 🔧 Unsupervised Feature-based Approaches {#unsupervised-feature-based-approaches}

* Learn fixed or contextual embeddings from unlabeled text **without fine-tuning**.
* Examples:

  * **Non-neural**: Brown Clustering.
  * **Neural**: Word2Vec, GloVe.
  * **Sentence-level**: Skip-Thought, Paragraph Vector.
  * **Contextual**: ELMo (context-sensitive using bidirectional LMs).
* Widely adopted to improve downstream NLP performance via pre-trained embeddings.

### 2.2 🧪 Unsupervised Fine-tuning Approaches {#unsupervised-fine-tuning-approaches}

* Pretrain language models on unlabeled data and **fine-tune** the entire model for specific tasks.
* Examples: **GPT**, **ULMFiT**.
* Benefits:

  * Reduces the need for large task-specific models.
  * Achieves high accuracy with fewer labeled examples.
  * Strong benchmark performance (e.g., **GLUE**).

### 2.3 🔁 Transfer Learning from Supervised Data {#transfer-learning-from-supervised-data}

* Leverages large **supervised datasets** (e.g., NLI, translation) to learn reusable representations.
* Transfers the learned knowledge to other tasks.
* Examples:

  * **NLP**: InferSent, CoVe.
  * **Vision**: ImageNet-pretrained CNNs.
* Effective when high-quality labeled data is available.

---

## 🧱 BERT Architecture

### 🔄 Two-Stage Framework {#two-stage-framework}

1. **Pre-training**: Train on large amounts of **unlabeled text** using:

   * Masked Language Modeling (MLM)
   * Next Sentence Prediction (NSP)
2. **Fine-tuning**: Initialize with pre-trained weights and **fine-tune** on labeled task-specific data. 🔁 **One model per task.**

<img src="{{ '/images/bert1.png' | relative_url }}">

### 🔍 Key Features {#key-features}

* **Unified architecture** across all NLP tasks.
* Eliminates need for task-specific design—just fine-tune with minimal modifications.

### 🧬 Model Configuration
<img src="{{ '/images/bert2.png' | relative_url }}">

* BERT-Base has the same size as the original **OpenAI GPT** model.
* Uses **WordPiece tokenizer** with a 30,000-token vocabulary.

### 🔡 Input Embedding Composition {#input-embedding-composition}

InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding

<img src="{{ '/images/bert3.png' | relative_url }}">

---

## 🧱 **Pre-training BERT**

BERT is pre-trained on two **unsupervised tasks** designed to help the model learn deep bidirectional representations:

### 🔹 **Task 1: Masked Language Modeling (MLM)**

* Randomly **mask 15%** of the input tokens.
* Of the selected tokens:

  * **80%** are replaced with the special [MASK] token.
  * **10%** are replaced with a **random word**.
  * **10%** are **left unchanged**.
* **Objective**: For each masked token, the model generates a hidden representation → passes it through a softmax layer → predicts the **original token**.

📊 This encourages the model to **use both left and right context**, enabling **bidirectional understanding**.

<img src="{{ '/images/bert4.png' | relative_url }}">



### 🔹 **Task 2: Next Sentence Prediction (NSP)**

* Designed to capture **inter-sentence relationships**, critical for tasks like **question answering** and **natural language inference (NLI)**.
* For each training pair:

  * **50%** of the time, Sentence B **follows** Sentence A (labeled as *IsNext*).
  * **50%** of the time, Sentence B is a **random sentence** from the corpus (labeled as *NotNext*).
* **Objective**: Perform **binary classification** to predict whether Sentence B is the actual next sentence after Sentence A.

🧾 **Input Format**:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```


### 📚 **Pre-training Dataset**

BERT was pre-trained on two large-scale text corpora:

* **BooksCorpus** (\~800 million words)
* **English Wikipedia** (\~2,500 million words) : Only text passages were used—tables, lists, and headers were excluded.*

---

## 🧱 **Fine-tuning BERT**

Fine-tuning involves taking the **pre-trained BERT model** and training it further on a **specific downstream task** using labeled data.


### ✅ **Why Fine-tuning BERT Is Simple**

BERT’s architecture is **task-agnostic**, making fine-tuning straightforward:

* Supports both:

  * **Single-sentence tasks** (e.g., sentiment classification)
  * **Sentence-pair tasks** (e.g., question answering, sentence similarity)
* No need to redesign the entire model.
* Just:

  * **Modify the input format** as needed
  * **Add a task-specific output layer**
  * **Train the entire model end-to-end**

This modularity allows quick adaptation to a wide variety of NLP tasks with minimal changes.


### 🧾 **Input & Output Setup**

<img src="{{ '/images/bert5.png' | relative_url }}">

BERT handles input as a single packed sequence:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

* `[CLS]` token's final hidden state is typically used for classification tasks.
* `[SEP]` tokens mark sentence boundaries in pair-input tasks.


### 💰 **Fine-tuning Efficiency**

* Fine-tuning BERT is **computationally inexpensive** compared to pre-training.
* All results reported in the original BERT paper can be reproduced:

  * In **under 1 hour** on a Cloud TPU
  * Or in **a few hours** on a single modern GPU

---
