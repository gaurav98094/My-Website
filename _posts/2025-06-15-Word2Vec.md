---
layout: post
title: "Efficient Estimation of Word Representations in Vector Space"
date: 2025-06-15
categories: papers
tags: [embeddings]
description: "Introducing CBOW and Skip-Gram problem."
image: /images/image1.png
---

<img src="{{ '/poster/word2vec.png' | relative_url }}">




# Efficient Estimation of Word Representations in Vector Space - Word2Vec

## Abstract
* Proposes **two new model architectures** for learning **continuous word vectors** from very large datasets.
* Evaluates the word vectors using a **word similarity task**.
* Achieves **significant accuracy improvements** over previous neural network-based methods.
* Models are **computationally efficient** — can train on **1.6 billion words in under a day**.
* Learned vectors show **state-of-the-art performance** on tasks measuring both **syntactic and semantic similarities** between words.


## Introduction

* **Problem in NLP**:

  * Many NLP systems treat words as **discrete, atomic symbols (IDs)**—ignoring word similarity.
  * Simple models like **N-grams** are effective when trained on massive data, but have **limitations**, especially with domain-specific or limited datasets.

* **Limitations of Simple Models**:

  * In fields like **speech recognition** or **machine translation**, there isn’t enough high-quality, in-domain data.
  * Simply scaling up doesn’t help much—**advanced techniques** are needed.

* **Rise of Distributed Representations**:

  * **Neural network-based language models (NNLMs)** outperform N-gram models.
  * These models use **distributed (continuous) word representations**, capturing **semantic and syntactic regularities**.

* **Main Contribution of the Paper**:

  * Proposes new **efficient architectures** to learn **high-quality word vectors** from **very large corpora** (billions of words, millions of vocab items).
  * Existing models struggle to scale beyond a few hundred million words and low vector dimensions (50–100).

* **Goal**:

  * Develop architectures that capture **linear regularities** in word vectors (e.g., *King – Man + Woman = Queen*).
  * Create a **comprehensive test set** to evaluate **syntactic and semantic regularities**.

* **Historical Context**:

  * Early NNLMs jointly learned language models and word vectors using feedforward NNs.
  * Later approaches decoupled word vector learning from language modeling.
  * This paper builds on those ideas—focusing purely on **learning word vectors efficiently**.

* **Impact of Word Vectors**:

  * Word embeddings have improved many NLP tasks.
  * However, prior models were **computationally expensive**; this paper offers a **more scalable solution**.





##  Model Architectures

**General Overview**

* Focuses on **neural network-based methods** for learning **distributed word representations**, which outperform traditional techniques like **LSA** and **LDA** in preserving **linear regularities**.

* Introduces a **computational complexity metric**:
  <i>Total Cost = E × T × Q</i>

  * *E* = number of epochs
  * *T* = training set size
  * *Q* = model-specific complexity per example

* Uses **hierarchical softmax** with **Huffman trees** to reduce output layer complexity to around <i>log₂(V)</i> (instead of evaluating all V classes directly).


**Feedforward Neural Net Language Model (NNLM)**

* Architecture:

  * **Input layer**: 1-hot encoded previous N words
  * **Projection layer**: Shared matrix → size <i>N × D</i>
  * **Hidden layer**: Fully connected (typically 500–1000 units)
  * **Output layer**: Softmax over entire vocabulary *V*

* **Computational complexity** per training example:
  <i>Q = N × D + N × D × H + H × V</i>

  * Dominated by **H × V** term (output layer)

* Optimization:

  * Use **hierarchical softmax** or **class-based softmax** to reduce output cost.



**Recurrent Neural Net Language Model (RNNLM)**

* Motivation: Overcomes NNLM's limitation of **fixed context size (N)**.

* Architecture:

  * No projection layer
  * Input → Hidden (with **recurrent connections**) → Output
  * Can represent **sequential memory** via hidden state.

* **Computational complexity**:
  <i>Q = H × H + H × V</i>

  * Expensive due to recurrent matrix (**H × H**) and output (**H × V**)


**Parallel Training with DistBelief**

* Implemented models using **DistBelief** (Google’s distributed learning framework).
* Uses **multiple model replicas** in parallel.
* Gradient updates are **asynchronous** and shared via a **central parameter server**.
* Optimization: **Mini-batch SGD + Adagrad** for adaptive learning rates.
* Typical setup: **100+ replicas**, each using **multiple CPU cores** across a cluster.



## New Log-liner Models



* Goal: Develop **simpler and faster** architectures by **removing the non-linear hidden layer**, which is the main source of computational complexity in NNLMs.
* Inspired by a **two-step training approach**:

  1. Learn word vectors with a simple model.
  2. Train a language model (e.g., NNLM) on top of them.
* Advantage: Can be trained on **much larger datasets** than full neural nets.


<img src="{{ '/images/word2vec/image1.png' | relative_url }}">



**Continuous Bag-of-Words (CBOW) Model**

* **Key idea**: Predict the **current word** using its **surrounding context words** (history + future).
* **Architecture**:

  * Removes hidden layer.
  * Uses a **shared projection matrix** for all input words.
  * Averages context word vectors → projects into a **single representation**.
  * Trains a **log-linear classifier** to predict the middle word.
* **Ignores word order** → like a **bag-of-words**, but uses continuous vectors.
* **Training complexity**:
    <i>Q = N × D + D × log₂(V)</i>

  * N = number of context words
  * D = dimension of word vectors
  * V = vocabulary size
* Denoted as **CBOW**.



**Continuous Skip-gram Model**

* **Key idea**: Use the **current word** to predict multiple **context words** within a certain window.
* Opposite of CBOW (context → word); here it’s **word → context**.
* For each word, randomly sample C nearby context words.
* Distant words are given **less weight** (sampled less often).
* **Training complexity**:
  <i>Q = C × (D + D × log₂(V))</>

  * C = window size / context range
* More effective than CBOW in capturing **semantic relationships**, but slightly more **computationally expensive**.


## Implementation

```
import torch
import torch.nn as nn
import torch.optim as optim
from collections import defaultdict
import random

# Hyperparameters
CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right
EMBEDDING_DIM = 50
EPOCHS = 100
LR = 0.01

# Sample corpus
corpus = [
    "we are learning natural language processing",
    "natural language processing is fun",
    "we love deep learning and neural networks",
    "deep learning is a part of machine learning"
]

# Preprocessing: tokenize and build vocabulary
tokenized = [sentence.lower().split() for sentence in corpus]
vocab = set(word for sentence in tokenized for word in sentence)
word2idx = {word: idx for idx, word in enumerate(vocab)}
idx2word = {idx: word for word, idx in word2idx.items()}
vocab_size = len(vocab)

# Generate CBOW training pairs
def generate_cbow_data(tokenized_sentences, window_size):
    data = []
    for sentence in tokenized_sentences:
        for i in range(window_size, len(sentence) - window_size):
            context = (
                sentence[i - window_size:i] + sentence[i + 1:i + window_size + 1]
            )
            target = sentence[i]
            data.append((
                [word2idx[w] for w in context],
                word2idx[target]
            ))
    return data

training_data = generate_cbow_data(tokenized, CONTEXT_SIZE)

# CBOW Model
class CBOW(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, context_idxs):
        embeds = self.embeddings(context_idxs)           # (batch_size, context_size, embed_dim)
        mean_embed = embeds.mean(dim=1)                 # (batch_size, embed_dim)
        out = self.linear(mean_embed)                   # (batch_size, vocab_size)
        return out

# Prepare data for training
def prepare_batch(data):
    contexts = torch.tensor([c for c, _ in data], dtype=torch.long)
    targets = torch.tensor([t for _, t in data], dtype=torch.long)
    return contexts, targets

# Model, Loss, Optimizer
model = CBOW(vocab_size, EMBEDDING_DIM)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

# Training loop
for epoch in range(EPOCHS):
    total_loss = 0
    random.shuffle(training_data)
    for context, target in training_data:
        context_tensor = torch.tensor([context], dtype=torch.long)
        target_tensor = torch.tensor([target], dtype=torch.long)

        # Forward
        output = model(context_tensor)
        loss = loss_fn(output, target_tensor)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}")

# Get learned embeddings
embeddings = model.embeddings.weight.data

# Example: print embedding for a word
word = "learning"
print(f"Embedding for '{word}':\n{embeddings[word2idx[word]]}")
```