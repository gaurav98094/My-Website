---
layout: post
title: "BERT â€” Bidirectional Transformers for Language Understanding"
date: 2025-05-31
categories: papers
tags: [bert, pretraining, masked-language-modeling, nlp]
description: "A deep dive into BERT, the transformer model that revolutionized language understanding."
image: /images/image1.png
---

# BERT

<a href = "https://arxiv.org/pdf/1810.04805">Paper Link </a>
<a href = "https://jalammar.github.io/illustrated-bert/">Blog Post </a>


## ðŸ§  **Abstract**